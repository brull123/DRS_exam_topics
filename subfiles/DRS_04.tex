\documentclass[../exam_questions.tex]{subfiles}

\begin{document}
\lecture{Lecture 4}
\section{Define the Metzler matrix.}
Metzler matrix is a matrix, where all off-diagonal components are nonnegative, $M_{ij} \geq 0, \: i \neq j$.
A dynamical system modelled by a Metzler matrix  $\dot{x} = Mx$ has its dynamical trajectories contained in the positive orthant $\R^n_{+}$, if the initial values are from the orthant.\\
The matrix can be written as
\begin{align}
	 & M = -sI + A,\: \text{where}   \\
	 & s\in \R, s\geq 0, A\succeq 0.
\end{align}
\section{Define the M-matrix.}
The Z-matrix has all off-diagonal elements non-positive, $Z_{ij} \leq 0, \: i \neq j$.\\
$-Z$ is Metzler.

A M-matrix is a Z-matrix with all eigenvalues having non-negative real parts.
% \begin{itemize}
% 	\item A Z-matrix $E = sI - A$ is singular M-matrix if all its principal minors are nonnegative
% 	\item A Z-matrix $E = sI - A$ is nonsingular M-matrix if all its principal minors are positive
% 	\item If $E$ is a singular M-matrix, then $s\geq \rho(A)$ and $-E$ is a Metzler matrix
% 	\item $\dots$
% \end{itemize}


\section{Explain how the geometric multiplicity of eigenvalues differ from the algebraic one.}
\subsection{Algebraic multiplicity}
\begin{definition}{Algebraic multiplicity of an eigenvalue}{}
	Algebraic multiplicity of the eigenvalue $\lambda$ is how many times it appears as a root of the characteristic polynomial
	\begin{align}
		p(s) & = \det\left(sI - A\right).
	\end{align}
\end{definition}

\subsection{Geometric multiplicity}
\begin{definition}{Geometric multiplicity of an eigenvalue}{}
	Geometric multiplicity of the eigenvalue $\lambda$ is the dimension of the eigenspace
	\begin{align}
		gm(\lambda) & = \dim \left(\ker \left(A - I \lambda\right) \right).
	\end{align}
	It gives the amount of linearly independent eigenvectors associated with the eigenvalue $\lambda$.
\end{definition}

\section{Explain the meaning of the geometric multiplicity of a zero eigenvalue for graph Laplacians.}
By construction, the Laplacian matrix always has at least one zero eigenvalue. The amount of zero eigenvalues of the graph Laplacian is the amount of spanning trees in the spanning forest of the graph = number of strongly connected components of the graph.

\section{Define the left zero eigenvector.}
For any graph
\begin{align}
	L & = D - A,
\end{align}
then
\begin{align}
	L \bm{1} & = 0.
\end{align}
There must also exist
\begin{align}
	\exists p^T, \: p^T L & = 0,
\end{align}
a left eigenvector corresponding to the eigenvalue 0.
The elements $p_i$ of the left eigenvector $p$ are nonnegative
\begin{align}
	p_i \geq 0, \quad p \succeq 0.
\end{align}
The elements which are greater than 0 correspond to nodes of the graph which can be roots of a spanning tree.
For a strongly connected graph $\forall p_i, \: p_i > 0$.


A weight-balanced graph has the following property
\begin{align}
	d_i^{in} & = d_i^{out} \\
	L        & = L^T.
\end{align}
Then it is true that
\begin{align}
	p^T        & = \bm{1}^T, \\
	\bm{1}^T L & = 0.
\end{align}

\section{Define a balanced graph.}
A balanced graph is a graph where the indegree and outdegree of each vertex are equal. An example of a balanced graph is an undirected graph.

\section{Define a strongly connected graph.}
A strongly connected graph is a graph where there exists a directed path between any 2 vertices of the graph.

\section{Define the degree centrality.}
Degree centrality is a measure of the graph.
\begin{align}
	d_i & = \sum_j A_{ij} = A \bm{1}
\end{align}

\section{Motivate the eigenvector centrality.}
\begin{definition}{Eigenvector centrality}{}
	Eigenvector centrality measures the importance of a node as a function of the importance of its neighbors. If a node is connected to
	highly important nodes, it will have a higher eigenvector centrality compared to the less important nodes.
	\begin{align}
		x_i' & = \sum_{j} A_{ij} x_j \\
		x'   & = A x
	\end{align}
	The algorithm of obtaining the eigenvector centrality of a graph is iterative.\\
	\begin{itemize}
		\item Choose an initial seed $x(0) = c^T V$, where $V$ is the matrix of eigenvectors of the adjacency matrix and $c$ is a weighing vector.
		\item Iterate
		      \begin{align}
			      x_i' & = \frac{1}{\lambda_1}\sum_j A_{ij} x_j,
		      \end{align}
		      where $\lambda_1$ is the maximal eigenvalue, the division is done to prevent blow-up or convergence to zero.
	\end{itemize}
\end{definition}

\section{Explain the difference between the eigenvector centrality and the Katz centrality.}
\begin{definition}{Katz centrality}{}
	Generalization of the eigenvector centrality
	\begin{align}
		x_i & = \alpha \sum_j A_{ij} x_j +\beta,
	\end{align}
	which allows to calculate the centrality in a non-iterative manner
	\begin{align}
		x           & = \left(I - \alpha A\right)^{-1}\bar{\beta} \\
		\bar{\beta} & = \bm{1}\beta.
	\end{align}
\end{definition}
\end{document}