\documentclass[../exam_questions.tex]{subfiles}

\begin{document}

\lecture{Lecture 11} % Video lecture 9
\section{Describe the variant Kernighan-Lin algorithm for community detection. What is its complexity? How does it compare to the original Kernighan-Lin algorithm for graph partitioning?}
In case of community detection, it is not possible to use the size of the cut set as a criterion as it would be always possible to set the size of one of the clusters to 0, rendering the cut set size also 0.\\
Instead, we consider how many edges between vertices belonging to same communities as compared to randomly expected number of connections. Such property is measured with modularity
\begin{align}
	Q & = \frac{1}{2m} \sum_{i,j}\left(A_{ij} - \frac{d_i d_j}{2m}\delta(c_i, c_j)\right) = \frac{1}{2m}\sum_{i,j} B_{ij}\delta(c_i, c_j).
\end{align}

This variant of the Kernighan-Lin is also a heuristic algorithm.
Divides the network into 2 clusters of vertices. Calculate the change of modularity when moving a vertex to the other group.
In each iteration choose the vertex which increases the modularity the most or decreases the least.
A vertex that has been moved cannot be moved in the same iteration. Then we choose the best resulting partition from which we start another iteration.

In each round $O(n)$ modularity changes are considered, for every vertex $O(m/n)$ possible connections to other vertices are evaluated. That is $O(m)$ per every iteration of the algorithm.
The amount of iterations is proportional to $O(n)$ so in total the
overall complexity is $O(nm)$. The original varian of the algorithm has complexity $O(n^2m)$.

\section{Describe the spectral modularity maximization method of community detection. Explain the importance of the modularity matrix and its leading eigenvector.}
\begin{warningbox}
	Proof/Derivation is required as a part of the exam
\end{warningbox}
The method is based on the modularity matrix
\begin{align}
	Q & = \frac{1}{2m} \sum_{i,j} A_{ij} - \frac{d_i d_j}{2m} \delta (c_i, c_j) = \frac{1}{2m} \sum_{i,j} B_{i,j} \delta(c_i, c_j),
\end{align}
where $c_i$ are the community labels.

By definition
\begin{align}
	\sum_{j} B_{ij} & = \sum_{j} A_{ij} - \frac{d_i}{2m}\sum_{j} d_j = 0, \: \text{}         \\
	\sum_{j} B_{ij} & = \sum_{j} A_{ij} - \frac{d_j}{2m}\sum_{i} d_i = d_j^{out} - d_j^{in},
\end{align}
which is also equal to zero on undirected and balanced graphs.

We define a vector $s \in \R^n$ such that
\begin{align}
	s_i & =
	\begin{cases}
		+1 \:\text{if}\: i \in \mathcal{V}_1, \\
		-1 \:\text{if}\: i \in \mathcal{V}_2.
	\end{cases}
\end{align}
With the help of $s_i$ vertex labels one can construct the expression
\begin{align}
	\frac{1}{2}(1 - s_i s_j) & =
	\begin{cases}
		1 \:\text{if} \:(i,j) \:\text{vertices are in different groups} \\
		0 \:\text{if} \:(i,j) \:\text{vertices are in the same group}.
	\end{cases}
\end{align}

Using this expression, we rewrite the original expression for modularity
\begin{align}
	Q & = \frac{1}{4m} \sum_{i,j} B_{ij} (s_i s_j +1) = \frac{1}{4m} \left(\sum_{i,j} B_{ij}s_i s_j + \sum_{i,j} B_{ij}\right) \\
	Q & = \frac{1}{4m} \sum_{i,j} B_{ij} s_i s_j = \frac{1}{4m} s^T B s.
\end{align}

The maximization is then a restricted quadratic optimization problem, which is difficult to solve, but we can relax the problem to a constrained optimization problem by
rather than considering $s_i = \pm 1$ being the vertices of a hyper-cube, we allow them to attain values on a hyper-sphere.
The diameter of the hyper-sphere is given by the constraint $s^Ts = \sum_i s_i^2 = n$ giving us the diameter $\sqrt(n)$.

The relaxed problem is a constrained unrestricted optimization problem, which can be optimized using the Lagrange method
\begin{align}
	\pdv{}{s_i} \left[\sum_{j,k} B_{jk}s_j s_k + \beta \left(n - \sum_j s_j^2\right)\right] & = 0, \: \forall i \\
	\sum_{j} B_{ij}s_j - \beta s_i                                                          & = 0,
\end{align}
which can be written in a compact version
\begin{align}
	B s & = \beta s.
\end{align}
This means that $\beta$ is an eigenvalue of $B$ and the modularity value is
\begin{align}
	Q & = \frac{1}{4m} s^T B s = \frac{1}{4m} \beta s^T s =\frac{1}{4m} \beta n.
\end{align}

The modularity value is then maximized by the leading eigenvector $u_1$ of the modularity matrix.
However, this is only the optimum of the relaxed problem. To find an optimum to the original problem, we have to choose $s_i = \pm 1$ nearest to the optimum of the relaxed problem.
This can be done by maximizing their product
\begin{align}
	s^T u_1 & = \sum_i s_i u_{1i},
\end{align}
by choosing
\begin{align}
	s_i & = \begin{cases}
		        +1, \: u_{1i} > 0 \\
		        -1, \: u_{1i} < 0.
	        \end{cases}
\end{align}
If an element of the leading eigenvector is equal to zero, either assignment is equally fine.

\subsection{Spectral modularity maximization algorithm}
\begin{itemize}
	\item Find the leading eigenvector of $B$ - $u_1 \in \R^n$
	\item Assign vertices to the two communities based on the sign of the elements of $u_1$
\end{itemize}

Unfortunately, the modularity matrix $B$ is never sparse as opposed to the Laplacian, finding the leading eigenvector is therefore $O(n^3)$.

\section{How does repeated bisection work for modularity maximization? Compare it with the same approach in graph partitioning.}
Repeated bisection allows for division into more than two communities.
The algorithm first separates the network into two communities. Then it runs again on each of the communities. This approach
is applicable in graph partitioning, but in the case of community detection, modularity maximization upon bisecting each community does not in general maximize the total modularity of the resulting network division. We must therefore consider the resulting change in the total modularity.

For a community $C$ of $n_c$ vertices, the change in the modularity of the entire network upon bisection of $C$ equals
\begin{align}
	\Delta Q & = \frac{1}{4m} s^T B^C s,\:{\rm where}       \\
	B_{ij}^C & = B_{ij} - \delta_{ij} \sum_{k\in C} B_{ik}.
\end{align}
We can now use the spectral modularity maximization is applicable on modularity change matrix $B^C$.
We continue bisection of communities as long as modularity increase is possible.


\section{Briefly describe simulated annealing, genetic algorithm and greedy algorithm for modularity maximization.}
\subsection{Simulated annealing}
A stochastic optimization approach used for maximization of the modularity. The method finds approximately maximal $Q$ without getting stuck in a local maxima.
The algorithm uses the following steps
\begin{itemize}
	\item Change an assignment of a vertex
	\item Calculate the change in modularity $\Delta Q$
	\item If $\Delta Q \geq 0$, accept the change
	\item If $\Delta Q < 0$, accept the change with probability $p = \exp\left(\frac{\Delta Q}{T}\right)$, where $T$ is the "temperature"
	\item The "temperature" decays with time in:
	      \begin{itemize}
		      \item Geometric manner - $T_{k+1} = \alpha T_k$, where $\alpha in [0.9, 0.999]$
		      \item Linear manner
		      \item Adaptive manner
	      \end{itemize}
\end{itemize}

\subsection{Genetic algorithm}
\begin{itemize}
	\item Generate an initial seed with semi-random partitions
	\item Select which partitions continue into the next generation - modularity evaluation of partitions
	\item Crossover selected partitions
	\item Mutation - random variation keeps diversity
\end{itemize}


\subsection{Greedy algorithm}
Starts from individual nodes. Joins them together so that modularity is maximized.\\
In the next step it joins the communities from previous step.\\
Ends when the whole graph is a single community. Checks all intermediate states and finds the one with the largest modularity.\\
Only algorithm the sort of works on very large networks $O(n \log^2n)$.


\section{Describe the algorithm using betweenness centrality for community detection. How does the Radicci algorithm differ from it?}
Betweenness is measure how many shortest path between edges go through a node.
The vertices that connect communities tend to have higher betweenness centrality.

In each step remove the edge with a largest betweenness centrality. Recalculate betweenness centrality.
Repeat the steps until there are no edges remaining - choose which step had the highest modularity.

Complexity $O(mn^2 + nm^2)$. On a sparse graph $O(n^3)$

Radicci modification finds if an edge is a part of a short loop (4 edges).It has been empirically found, that edges connecting communities
are often not parts of short loops.
Works well if the graph has a lot of short loops (social networks). Technical networks usually avoid short loops.

Ends only after avery single vertex is separated. It is up to us to choose which division has maximal Modularity.

\section{Explain how the agglomerative algorithms proceed in community detection.}
\begin{itemize}
	\item Agglomerative algorithm starts with individual vertices and joins them into ever greater communities.
	\item An divisive method does the opposite.
	\item The algorithm requires a definition of a measure of similarity of groups of vertices
	      \begin{itemize}
		      \item First define similarity for pairs of vertices
		      \item Then generalize this measure for groups - maximal between any, minimal between any, average, etc.
	      \end{itemize}
\end{itemize}

\section{Define hierarchical clustering. What is a dendrogram? Explain the similarity-based hierarchical clustering with single-, complete- and average-linkage clustering.}
Dendogram - tree like structure which shows how the network is separated. In the root, the whole graph is a single cluster. In each
level, the cluster is separated into 2. The lower in the Dendogram, the more separated the network is.
It is then up to us to choose which division has maximal Modularity.

Betweenness centrality and Radicci methods started with the whole graph and gradually separated the network into single vertices. Similarity based
clustering starts with single vertices and connects them with most similar. Then it does the same for the created communities (tuples of vertices) etc.
The algorithm ends with the whole network in a single community. Again, we then evaluate at which level of the Dendogram was the modularity maximal.

For similarity clustering we need to define similarity of 2 vertices - (for example hierarchical equivalence, structural equivalence)
and then we need to generalize how to calculate similarity for communities:
\begin{itemize}
	\item minimal similarity of 2 vertices each in one cluster - $O(1)$
	\item maximum similarity of 2 vertices each in one cluster - $O(1)$
	\item average similarity - relatively easy to recalculate - $O(1)$
	\item etc.
\end{itemize}
The $O(1)$ complexity can be achieved when storing previous similarities in a binary heap.
Calculating the average of larger communities does not need to sum every single vertex, we only need to average the similarities of the previous connection - $O(1)$
\begin{align}
	\bar{a}_1     & = \frac{1}{n_1} \sum a_i                                           \\
	\bar{a}_2     & = \frac{1}{n_2} \sum a_i                                           \\
	\bar{a}_{1,2} & = \frac{1}{n_1 + n_2} \left(n_1 \bar{a}_1 + n_2 \bar{a}_2 \right).
\end{align}
With the binary heap addition and removal take $O(n \log n)$. The total complexity is $O(n^2 \log n)$.

The result is usually tightly knit core and separate peripherals.

\section{Comment on which of the algorithms detect a fixed number vs. an unspecified number of communities.}
\subsection{Specified}
\begin{itemize}
	\item Kernihan-lin algorithm for modularity maximization
	\item Spectral Modularity maximization
\end{itemize}
\subsection{Unspecified}
\begin{itemize}
	\item Repeated bisection
	\item Simulated annealing
	\item Genetic algorithm
	\item Greedy algorithm
	\item Hierarchical clustering
\end{itemize}

\end{document}